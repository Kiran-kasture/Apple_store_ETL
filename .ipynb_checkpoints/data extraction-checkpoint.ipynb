{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a24fa4f-882e-4828-99b9-790dce9deab9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['Sale_Id']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m existing_sales_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(sales_file):\n\u001b[0;32m    107\u001b[0m     existing_sales_ids\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m--> 108\u001b[0m         \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msales_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSale_Id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSale_Id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    109\u001b[0m     )\n\u001b[0;32m    111\u001b[0m new_sales \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(DAILY_SALES):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[0;32m    139\u001b[0m ):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\base_parser.py:979\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    977\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    980\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    981\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    982\u001b[0m     )\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['Sale_Id']"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_DIR = os.path.join(os.getcwd(), \"datasets\")\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "fake = Faker(\"en_US\")   # Ensures U.S. locale for names & dates\n",
    "\n",
    "DAILY_SALES = 100_000   # Daily Sales to append\n",
    "DAILY_WARRANTY = 3_000  # Daily Warranty to append\n",
    "RESET_DAYS = 2          # Change seed every 2 days\n",
    "\n",
    "# ---------------- SEED ----------------\n",
    "seed_value = datetime.date.today().toordinal() // RESET_DAYS\n",
    "Faker.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# ---------------- Helper Functions ----------------\n",
    "def unique_sales_id(existing: set, letters=2, digits=(4, 6)):\n",
    "    \"\"\"Generate unique Sales_Id like AB-12345\"\"\"\n",
    "    while True:\n",
    "        n_digits = random.randint(*digits)\n",
    "        new_id = (\n",
    "            ''.join(random.choices(string.ascii_uppercase, k=letters))\n",
    "            + '-' + ''.join(random.choices(string.digits, k=n_digits))\n",
    "        )\n",
    "        if new_id not in existing:\n",
    "            existing.add(new_id)\n",
    "            return new_id\n",
    "\n",
    "def us_date(date_obj):\n",
    "    \"\"\"Return date in US format MM/dd/yyyy\"\"\"\n",
    "    return date_obj.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "# ---------------- Static CSVs ----------------\n",
    "categories_file = os.path.join(BASE_DIR, \"category.csv\")\n",
    "products_file   = os.path.join(BASE_DIR, \"products.csv\")\n",
    "stores_file     = os.path.join(BASE_DIR, \"stores.csv\")\n",
    "\n",
    "if not os.path.exists(categories_file):\n",
    "    categories = [\n",
    "        \"Laptop\",\"Audio\",\"Tablet\",\"Smartphone\",\"Wearable\",\n",
    "        \"Streaming Device\",\"Desktop\",\"Subscription Service\",\n",
    "        \"Smart Speaker\",\"Accessories\"\n",
    "    ]\n",
    "    pd.DataFrame(\n",
    "        [{\"Category_Id\": f\"CAT-{i+1}\", \"Category_Name\": c} for i,c in enumerate(categories)]\n",
    "    ).to_csv(categories_file, index=False)\n",
    "\n",
    "if not os.path.exists(products_file):\n",
    "    fake_static = Faker(\"en_US\")\n",
    "    apple_products = {\n",
    "        \"CAT-1\":[\"MacBook\",\"MacBook Air (M1)\",\"MacBook Air (M2)\",\"MacBook Pro 13-inch\",\"MacBook Pro 14-inch\"],\n",
    "        \"CAT-2\":[\"AirPods (2nd Gen)\",\"AirPods Pro\",\"AirPods Max\"],\n",
    "        \"CAT-3\":[\"iPad 10\",\"iPad Air\",\"iPad Pro\"],\n",
    "        \"CAT-4\":[\"iPhone 14\",\"iPhone 13\",\"iPhone SE\"],\n",
    "        \"CAT-5\":[\"Apple Watch Series 9\",\"Apple Watch Ultra\"],\n",
    "        \"CAT-6\":[\"Apple TV 4K\",\"Apple TV HD\"],\n",
    "        \"CAT-7\":[\"iMac 24\",\"Mac Pro\",\"Mac Mini\"],\n",
    "        \"CAT-8\":[\"iCloud\",\"Apple Music\",\"Apple TV+\"],\n",
    "        \"CAT-9\":[\"HomePod\",\"HomePod mini\"],\n",
    "        \"CAT-10\":[\"Magic Keyboard\",\"Magic Mouse\",\"AirTag\"]\n",
    "    }\n",
    "    rows, pid = [], 1\n",
    "    for cat_id, names in apple_products.items():\n",
    "        for name in names:\n",
    "            rows.append({\n",
    "                \"Product_ID\": f\"P-{pid}\",\n",
    "                \"Product_Name\": name,\n",
    "                \"Category_ID\": cat_id,\n",
    "                \"Launch_Date\": us_date(fake_static.date_this_decade()),\n",
    "                \"Price\": random.randint(100, 2000)\n",
    "            })\n",
    "            pid += 1\n",
    "    pd.DataFrame(rows).to_csv(products_file, index=False)\n",
    "\n",
    "if not os.path.exists(stores_file):\n",
    "    base_stores = [\n",
    "        (\"Apple Fifth Avenue\",\"New York\",\"United States\"),\n",
    "        (\"Apple Union Square\",\"San Francisco\",\"United States\"),\n",
    "        (\"Apple Michigan Avenue\",\"Chicago\",\"United States\"),\n",
    "        (\"Apple The Grove\",\"Los Angeles\",\"United States\"),\n",
    "        (\"Apple SoHo\",\"New York\",\"United States\")\n",
    "    ]\n",
    "    while len(base_stores) < 75:\n",
    "        base_stores.append(random.choice(base_stores))\n",
    "    pd.DataFrame([\n",
    "        {\"Store_ID\": f\"ST-{i+1}\", \"Store_Name\": s[0], \"City\": s[1], \"Country\": s[2]}\n",
    "        for i,s in enumerate(base_stores)\n",
    "    ]).to_csv(stores_file, index=False)\n",
    "\n",
    "# ---------------- Load IDs ----------------\n",
    "products_df = pd.read_csv(products_file)\n",
    "stores_df   = pd.read_csv(stores_file)\n",
    "product_ids = products_df[\"Product_ID\"].tolist()\n",
    "store_ids   = stores_df[\"Store_ID\"].tolist()\n",
    "\n",
    "# ---------------- SALES APPEND ----------------\n",
    "sales_file = os.path.join(BASE_DIR, \"sales.csv\")\n",
    "existing_sales_ids = set()\n",
    "if os.path.exists(sales_file):\n",
    "    existing_sales_ids.update(\n",
    "        pd.read_csv(sales_file, usecols=[\"Sales_Id\"])[\"Sales_Id\"].tolist()\n",
    "    )\n",
    "\n",
    "new_sales = []\n",
    "for _ in range(DAILY_SALES):\n",
    "    new_sales.append({\n",
    "        \"Sales_Id\": unique_sales_id(existing_sales_ids),\n",
    "        \"Sale_Date\": us_date(fake.date_between(start_date=\"-3y\", end_date=\"today\")),\n",
    "        \"Store_Id\": random.choice(store_ids),\n",
    "        \"Product_Id\": random.choice(product_ids),\n",
    "        \"Quantity\": random.randint(1, 10)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(new_sales).to_csv(\n",
    "    sales_file, mode=\"a\", index=False, header=not os.path.exists(sales_file)\n",
    ")\n",
    "print(f\"âœ… Added {DAILY_SALES:,} new sales rows\")\n",
    "\n",
    "# ---------------- WARRANTY APPEND ----------------\n",
    "warranty_file = os.path.join(BASE_DIR, \"warranty.csv\")\n",
    "existing_claim_ids = set()\n",
    "if os.path.exists(warranty_file):\n",
    "    existing_claim_ids.update(\n",
    "        pd.read_csv(warranty_file, usecols=[\"claim_id\"])[\"claim_id\"].tolist()\n",
    "    )\n",
    "\n",
    "all_sales_ids = pd.read_csv(sales_file, usecols=[\"Sales_Id\"])[\"Sales_Id\"].tolist()\n",
    "\n",
    "new_claims = []\n",
    "claim_counter = len(existing_claim_ids)\n",
    "for _ in range(DAILY_WARRANTY):\n",
    "    claim_counter += 1\n",
    "    claim_id = f\"CL_{claim_counter:04d}\"  # CL_0001, CL_0002...\n",
    "    existing_claim_ids.add(claim_id)\n",
    "    new_claims.append({\n",
    "        \"claim_id\": claim_id,\n",
    "        \"claim_date\": us_date(fake.date_between(start_date=\"-1y\", end_date=\"today\")),\n",
    "        \"sale_id\": random.choice(all_sales_ids),\n",
    "        \"repair_status\": random.choice([\"Pending\",\"In Progress\",\"Completed\",\"Rejected\"])\n",
    "    })\n",
    "\n",
    "pd.DataFrame(new_claims).to_csv(\n",
    "    warranty_file, mode=\"a\", index=False, header=not os.path.exists(warranty_file)\n",
    ")\n",
    "print(f\"âœ… Added {DAILY_WARRANTY:,} new warranty rows\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Daily append complete. Files are in:\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660de62-1fa1-4ebd-9192-728ec29ac02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241a066-5cd1-47e4-94b9-db93b0da1fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
